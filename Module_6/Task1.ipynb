{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marharyta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marharyta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/marharyta/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import pandas as pd\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create dataset using sklearn\n",
    "# 0 is business, 1 is entertainment, 2 is politics, 3 is sport, 4 is tech\n",
    "labels=[\"business\",\"entertainment\",\"politics\",\"sport\",\"tech\"]\n",
    "bbc_data = load_files(r\"/Users/marharyta/Desktop/Data_science/IT_Academy/Data_science_course/Module_6_NLP/Homework/bbc\")\n",
    "X, y = bbc_data.data, bbc_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  target  category\n0  b'Tate & Lyle boss bags top award\\n\\nTate & Ly...       0  business\n1  b\"Halo 2 sells five million copies\\n\\nMicrosof...       4      tech\n2  b'MSPs hear renewed climate warning\\n\\nClimate...       2  politics\n3  b'Pavey focuses on indoor success\\n\\nJo Pavey ...       3     sport\n4  b'Tories reject rethink on axed MP\\n\\nSacked M...       2  politics",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b'Tate &amp; Lyle boss bags top award\\n\\nTate &amp; Ly...</td>\n      <td>0</td>\n      <td>business</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b\"Halo 2 sells five million copies\\n\\nMicrosof...</td>\n      <td>4</td>\n      <td>tech</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b'MSPs hear renewed climate warning\\n\\nClimate...</td>\n      <td>2</td>\n      <td>politics</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b'Pavey focuses on indoor success\\n\\nJo Pavey ...</td>\n      <td>3</td>\n      <td>sport</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b'Tories reject rethink on axed MP\\n\\nSacked M...</td>\n      <td>2</td>\n      <td>politics</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert dataset to pandas df\n",
    "NUM_LABELS= len(labels)\n",
    "id2label={i:l for i,l in enumerate(labels)}\n",
    "label2id={l:i for i,l in enumerate(labels)}\n",
    "\n",
    "df = pd.DataFrame(data = X, columns = ['text'])\n",
    "df['target'] = y\n",
    "df[\"category\"]=df.target.map(lambda x: id2label[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#use regex to clean the text\n",
    "# text preprocessing, remove all the special characters using regex\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pymystem3 import Mystem\n",
    "stemmer = Mystem()\n",
    "def regex_preprocessing(X):\n",
    "    documents = []\n",
    "\n",
    "    for sen in range(len(X)):\n",
    "        # Remove all the special characters, only letters are left\n",
    "        document = str(X[sen])\n",
    "        document = document.rstrip()\n",
    "        document = re.sub(r'\\W', ' ', document)\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "        # Lemmatization\n",
    "        #document = document.split()\n",
    "        document = \"\".join(stemmer.lemmatize(document)).strip()\n",
    "\n",
    "\n",
    "        documents.append(document)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = regex_preprocessing(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  target  category  \\\n0  b'Tate & Lyle boss bags top award\\n\\nTate & Ly...       0  business   \n1  b\"Halo 2 sells five million copies\\n\\nMicrosof...       4      tech   \n2  b'MSPs hear renewed climate warning\\n\\nClimate...       2  politics   \n3  b'Pavey focuses on indoor success\\n\\nJo Pavey ...       3     sport   \n4  b'Tories reject rethink on axed MP\\n\\nSacked M...       2  politics   \n\n                                   preprocessed_text  \n0  tate lyle boss bags top award ntate lyle chief...  \n1  halo 2 sells five million copies nmicrosoft is...  \n2  msps hear renewed climate warning nclimate cha...  \n3  pavey focuses on indoor success njo pavey will...  \n4  tories reject rethink on axed mp nsacked mp ho...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n      <th>category</th>\n      <th>preprocessed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b'Tate &amp; Lyle boss bags top award\\n\\nTate &amp; Ly...</td>\n      <td>0</td>\n      <td>business</td>\n      <td>tate lyle boss bags top award ntate lyle chief...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b\"Halo 2 sells five million copies\\n\\nMicrosof...</td>\n      <td>4</td>\n      <td>tech</td>\n      <td>halo 2 sells five million copies nmicrosoft is...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b'MSPs hear renewed climate warning\\n\\nClimate...</td>\n      <td>2</td>\n      <td>politics</td>\n      <td>msps hear renewed climate warning nclimate cha...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b'Pavey focuses on indoor success\\n\\nJo Pavey ...</td>\n      <td>3</td>\n      <td>sport</td>\n      <td>pavey focuses on indoor success njo pavey will...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b'Tories reject rethink on axed MP\\n\\nSacked M...</td>\n      <td>2</td>\n      <td>politics</td>\n      <td>tories reject rethink on axed mp nsacked mp ho...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# now use TFIDF to convert words to vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = tfidfconverter.fit_transform(regex_preprocessing(X)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# now we are ready for classical machine learning\n",
    "# test-train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# quality report\n",
    "def quality_report(prediction, actual):\n",
    "    print(\"Accuracy: {:.3f}\\nPrecision: {:.3f}\\nRecall: {:.3f}\\nf1_score: {:.3f}\".format(\n",
    "        accuracy_score(actual, prediction),\n",
    "        precision_score(actual, prediction, average='micro'),\n",
    "        recall_score(actual, prediction, average='micro'),\n",
    "        f1_score(actual, prediction, average='micro')\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train quality:\n",
      "Accuracy: 0.996\n",
      "Precision: 0.996\n",
      "Recall: 0.996\n",
      "f1_score: 0.996\n",
      "\n",
      "Test quality:\n",
      "Accuracy: 0.971\n",
      "Precision: 0.971\n",
      "Recall: 0.971\n",
      "f1_score: 0.971\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Train quality:\")\n",
    "quality_report(log_reg.predict(X_train), y_train)\n",
    "\n",
    "print(\"\\nTest quality:\")\n",
    "quality_report(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# inference\n",
    "text = [\"One year ago, on the same day Lewis Hamilton was being crowned BBC Sports Personality of the Year for 2020, Emma Raducanu swept aside Grace Piper to win the BT Masters title at the National Tennis Centre in Roehampton. Afterwards Raducanu, then ranked 345th in the world and yet to make her main WTA tour debut, provided a glimpse of the steely resolve that, 12 months on, would see her succeed Hamilton to put her own name on the prestigious trophy.\"]\n",
    "text_preproc = regex_preprocessing(text)\n",
    "text_transformed = tfidfconverter.transform(text_preproc).toarray()\n",
    "my_prediction = log_reg.predict(text_transformed)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport\n"
     ]
    }
   ],
   "source": [
    "# make class prediction\n",
    "x = int(my_prediction)\n",
    "label = id2label[x]\n",
    "print(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train quality:\n",
      "Accuracy: 1.000\n",
      "Precision: 1.000\n",
      "Recall: 1.000\n",
      "f1_score: 1.000\n",
      "\n",
      "Test quality:\n",
      "Accuracy: 0.953\n",
      "Precision: 0.953\n",
      "Recall: 0.953\n",
      "f1_score: 0.953\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "random_classifier.fit(X_train, y_train)\n",
    "y_pred = random_classifier.predict(X_test)\n",
    "\n",
    "print(\"Train quality:\")\n",
    "quality_report(random_classifier.predict(X_train), y_train)\n",
    "\n",
    "print(\"\\nTest quality:\")\n",
    "quality_report(random_classifier.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5748fa77fc3b4c089b9c18b6484d9bd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# using DistilBERT\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "#from transformers import BertForSequenceClassification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=NUM_LABELS, id2label=id2label, label2id=label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('bert-base-uncased', model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "SIZE= df.shape[0]\n",
    "\n",
    "train_texts= list(df.preprocessed_text[:SIZE//2])\n",
    "val_texts=   list(df.preprocessed_text[SIZE//2:(3*SIZE)//4 ])\n",
    "test_texts=  list(df.preprocessed_text[(3*SIZE)//4:])\n",
    "\n",
    "train_labels= list(df.target[:SIZE//2])\n",
    "val_labels=   list(df.target[SIZE//2:(3*SIZE)//4])\n",
    "test_labels=  list(df.target[(3*SIZE)//4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tate lyle boss bags top award ntate lyle chief executive has been named european businessman of the year by leading business magazine niain ferguson was awarded the title by us publication forbes for returning one of the uk venerable manufacturers to the country top 100 companies the sugar group had been absent from the ftse 100 for seven years until mr ferguson helped it return to growth tate shares have leapt 55 this year boosted by firming sugar prices and sales of its artificial sweeteners n after years of sagging stock price and seven year hiatus from the ftse 100 one of britain venerable manufacturers has returned to the vaunted index forbes said mr ferguson took the helm at the company in 2003 after spending most of his career at consumer goods giant unilever tate lyle which was an original member of the historic ft 30 index in 1935 operates more than 41 factories and 20 more additional production facilities in 28 countries previous winners of the forbes award include royal bank of scotland chief executive fred goodwin and former vodafone boss chris gent\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#check if everything is alright\n",
    "print(train_texts[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1112, 556, 557)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts), len(val_texts), len(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings  = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_encodings, train_labels)\n",
    "val_dataset = MyDataset(val_encodings, val_labels)\n",
    "test_dataset = MyDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([  101, 17201,  1016, 15187,  2274,  2454,  4809, 22484, 26775, 19137,\n          6199,  2003, 12964, 21519,  4341,  1997,  2049, 12202, 16596, 10882,\n         13108, 17201,  1016, 23961,  5369,  2208,  2038,  2853,  2062,  2084,\n          2274,  2454,  4809,  4969,  2144,  2009,  2253,  2006,  5096,  1999,\n          3054,  2281,  1996,  2194,  2056, 17201,  1016,  2038,  4928,  2759,\n          3784,  2007, 27911,  2015, 18624,  2075,  2039,  2501,  2654,  2454,\n          2847,  2652,  1996,  2208,  2006, 12202,  2444,  2429,  2000,  7513,\n          3157,  2041,  1997,  2184, 12202,  2444,  2372,  2031,  2209,  1996,\n          2208,  2005,  2019,  2779,  1997,  6205,  2781,  2566,  5219, 23961,\n          5369,  8297,  2000,  1996,  2190,  4855,  2342,  2005,  3177,  5230,\n          2038, 25330,  3805,  1997,  1996,  2971,  2000,  2202,  1996,  2327,\n         10453,  1999,  1996,  2880,  2866,  2399,  6093,  1996,  3868,  2208,\n          2333,  2039,  2028,  3962,  2000,  2034,  2173, 16371,  2094,  4726,\n         14181,  2050,  2624, 12460,  2091,  2000,  2117,  2173, 17201,  1016,\n          3333,  2028,  2173,  2000,  2274,  2096,  2431,  2166,  1016,  3062,\n          2000,  2193,  3157,  2197,  2733,  2047,  7085,  3585, 17683, 12406,\n          4005,  1998,  3102, 15975,  2119,  3478,  2000,  2191,  2009,  2046,\n          1996,  2327,  2184, 24469,  2012,  2193,  2340,  1998,  2260,  4414,\n         17212,  8586,  8551,  3616,  1997,  2162, 10419,  4599,  2024,  9853,\n          1999,  1996,  2399,  3784,  2088,  2006,  1996,  3098,  2154,  1997,\n          1996,  2088,  1997,  2162, 10419,  5294,  4800,  2447,  3784,  2208,\n          2062,  2084,  3263,  2199,  2867,  2772,  2039,  2000,  2377,  2006,\n          1996,  3944,  1997,  1996,  2034,  2154,  2062,  2084,  2531,  2199,\n          2867,  2020,  1999,  1996,  2088,  6932, 21689,  2000,  5587,  2178,\n          4090, 14903,  2000, 11997,  2007,  1996, 18050,  1996,  3784,  2208,\n          4332,  1996,  3233,  2894,  2162, 10419,  2399,  2046, 14516,  2088,\n          2008,  2867,  2064, 21490,  2025,  2074,  3942, 11265, 10976,  5051,\n         27911,  2015,  2071,  2022,  3403,  2127,  2254,  2000,  2963,  2043,\n          2027,  2064,  2131,  2037, 10210,  3215,  2006, 10022, 27291,  5080,\n         10022, 16233,  2758,  2399, 22254, 19966,  2854, 12170,  2480,  2585,\n         27158,  2669, 10022,  2866,  2236,  3208,  2409,  2811,  3034,  2000,\n          2298,  2041,  2005,  4751,  1999,  1996,  2047,  2095,  2049,  2149,\n          4888,  2001,  2006,  4465,  1998,  2009,  3632,  2006,  5096,  1999,\n          2900,  2006,  1016,  2285, 10022,  2038,  5345,  3745,  1997,  1996,\n         27291, 10355,  3006,  1998,  2056,  2009,  3517,  2000,  5271,  2105,\n          2274,  2454,  1997,  1996, 16233,  2011,  2233,  2384,   102,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'labels': tensor(4)}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written\n",
    "    output_dir='./../outputs/TBERT/',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    #  The number of epochs, defaults to 3.0\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    # Number of steps used for a linear warmup\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "   # TensorBoard log directory\n",
    "    logging_dir='./../outputs/multi-class-logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    # save_strategy=\"epoch\",\n",
    "    #fp16=True,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    # the pre-trained model that will be fine-tuned\n",
    "    model=model,\n",
    "     # training arguments that we defined above\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics= compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marharyta/Desktop/Data_science/IT_Academy/Data_science_course/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1112\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 210\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/210 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "q=[trainer.evaluate(eval_dataset=data) for data in [train_dataset, val_dataset, test_dataset]]\n",
    "pd.DataFrame(q, index=[\"train\",\"val\",\"test\"]).iloc[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inference\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    return probs, probs.argmax(),model.config.id2label[probs.argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text = \"liam gallagher enjoying the champions league final saturday sporting social\"\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}